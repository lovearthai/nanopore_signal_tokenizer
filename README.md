# ğŸ§¬ Nanopore Signal Tokenizer

> å°† Nanopore åŸå§‹ç”µæµä¿¡å·ï¼ˆ5 kHzï¼‰è½¬æ¢ä¸ºç¦»æ•£ token åºåˆ—ï¼Œç”¨äºä¸‹æ¸¸è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰å»ºæ¨¡ DNA/RNA åºåˆ—ã€‚

[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

---

## ğŸ” ç®€ä»‹

æœ¬å·¥å…·åŸºäº **è‡ªç›‘ç£æ®‹å·®çŸ¢é‡é‡åŒ–ï¼ˆResidual VQï¼‰** æ¨¡å‹ï¼Œå°† Nanopore æµ‹åºä»ªè¾“å‡ºçš„åŸå§‹ç”µæµä¿¡å·ï¼ˆå•ä½ï¼špAï¼‰ç›´æ¥ tokenize ä¸ºç»“æ„åŒ–ç¦»æ•£ç¬¦å·åºåˆ—ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š`<|bwav:L1_5336|><|bwav:L2_7466|><|bwav:L3_6973|><|bwav:L4_6340|>`


- æ”¯æŒ **å¤šå±‚çº§ token è¾“å‡º**ï¼ˆL1 ~ L4ï¼‰ï¼Œå¯çµæ´»ç”¨äºä¸åŒç²’åº¦çš„å»ºæ¨¡ä»»åŠ¡  
- å…¼å®¹ **FAST5 æ–‡ä»¶** å’Œ **åŸå§‹æµ®ç‚¹ä¿¡å·æ•°ç»„**  
- å†…ç½® **ä¿¡å·å½’ä¸€åŒ– + Butterworth æ»¤æ³¢**ï¼Œæå‡é²æ£’æ€§  
- æ”¯æŒ **é•¿ä¿¡å·åˆ†å—å¤„ç†**ï¼ˆsliding window with overlapï¼‰

é€‚ç”¨äºï¼š
- Nanopore ä¿¡å·è¯­è¨€å»ºæ¨¡ï¼ˆSignal LMï¼‰
- æ— å‚è€ƒåºåˆ—çš„ RNA/DNA è¡¨å¾å­¦ä¹ 
- å¤šæ¨¡æ€ç”Ÿç‰©ä¿¡æ¯å­¦ pipeline æ„å»º

---

## âš™ï¸ å®‰è£…

### ä»æºç å®‰è£…ï¼ˆæ¨èï¼‰

```bash
git clone https://github.com/lovearthai/nanopore_signal_tokenizer.git
cd nanopore_signal_tokenizer
pip install -e .

##  å¿«é€Ÿå¼€å§‹

###  åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
å°†ä½ çš„ checkpointï¼ˆå¦‚ nanopore_rvq_tokenizer_chunk12k.pthï¼‰æ”¾å…¥ models/ ç›®å½•ã€‚

### Tokenize æ¨¡æ‹Ÿä¿¡å·

```python
# example_tokenize_data.py
import numpy as np
from nanopore_signal_tokenizer import RVQTokenizer

tokenizer = RVQTokenizer(
    model_ckpt="models/nanopore_rvq_tokenizer_chunk12k.pth",
    device="cuda:0",
    cutoff=1200,
    chunk_size=12000,
    downsample_rate=12
)

# æ¨¡æ‹Ÿä¸€æ®µ 1200 ç‚¹çš„ä¿¡å·ï¼ˆ~240ms @ 5kHzï¼‰
signal = np.random.randn(1200).astype(np.float32) * 5 + 100

# è·å–å…¨éƒ¨å±‚çº§ token (L1â€“L4)
tokens_all = tokenizer.tokenize_data(signal, fs=5000)
print(tokens_all)
# <|bwav:L1_5336|><|bwav:L2_7466|><|bwav:L3_6973|><|bwav:L4_6340|>...

# ä»…è·å– L1 å±‚
tokens_L1 = tokenizer.tokenize_data(signal, token_type="L1", fs=5000)
print(tokens_L1)
# <|bwav:L1_5336|><|bwav:L1_434|><|bwav:L1_4037|>...
```

### Tokenize FAST5 æ–‡ä»¶

```python
tokenizer.tokenize_fast5_file(
    fast5_path="sample.fast5",
    output_path="output.jsonl.gz"
)
```
è¾“å‡ºä¸º gzip å‹ç¼©çš„ JSONL æ ¼å¼ï¼š
```
{"id": "read_12345", "text": "<|bwav:L1_123|><|bwav:L2_456|>..."}
{"id": "read_67890", "text": "<|bwav:L1_789|><|bwav:L2_012|>..."}
```

##  é…ç½®å‚æ•°è¯´æ˜
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `model_ckpt` | é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ | å¿…å¡« |
| `device` | æ¨ç†è®¾å¤‡ | `"cuda"` |
| `cutoff` | æ»¤æ³¢æˆªæ­¢é¢‘ç‡ (Hz) | `1200` |
| `filter_order` | Butterworth æ»¤æ³¢å™¨é˜¶æ•° | `6` |
| `default_fs` | é»˜è®¤é‡‡æ ·ç‡ (Hz) | `5000` |
| `chunk_size` | æ¨¡å‹è¾“å…¥é•¿åº¦ï¼ˆå¿…é¡»ä¸è®­ç»ƒä¸€è‡´ï¼‰ | `12000` |
| `stride` | åˆ†å—æ»‘åŠ¨æ­¥é•¿ï¼ˆç”¨äºé•¿ readï¼‰ | `11880` |
| `discard_feature` | æ¯å—ä¸¤ç«¯ä¸¢å¼ƒçš„ token æ•°ï¼ˆé˜²è¾¹ç•Œæ•ˆåº”ï¼‰ | `0` |
| `downsample_rate` | ç¼–ç å™¨æ€»ä¸‹é‡‡æ ·ç‡ | `12` |

> âœ… `token_type`ï¼ˆéåˆå§‹åŒ–å‚æ•°ï¼Œç”¨äº `tokenize_data` / `tokenize_read`ï¼‰å¯é€‰ï¼š`"L1"`, `"L2"`, `"L3"`, `"L4"`ï¼ˆé»˜è®¤ `"L4"`ï¼‰
