import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import csv  # âœ… æ–°å¢ï¼šç”¨äºå†™å…¥ CSV
import time  # ç¡®ä¿å·²å¯¼å…¥
# ç›¸å¯¹å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from .dataset import NanoporeSignalDataset
from .vq_model import NanoporeVQModel
from typing import Dict, List
import collections
from .dwa import DynamicWeightAverager 

def log_and_save(
    epoch: int,
    step: int,
    total_epochs: int,
    total_steps: int,
    epoch_start_time: float,          # â† æ›¿æ¢ elapsed_time / remaining_time
    epoch_total_steps: int,           # â† å½“å‰ epoch çš„æ€»æ­¥æ•°ï¼ˆç”¨äºä¼°ç®—å‰©ä½™æ—¶é—´ï¼‰
    avg_recon_loss: float,
    avg_total_loss: float,
    avg_comit_loss: float,
    avg_diver_loss: float,
    avg_ortho_loss: float,
    codebook_usage: float,
    loss_csv_path: str,
    dynamic_recon_weight: float,
    dynamic_comit_weight: float,
    dynamic_ortho_weight: float,
    dynamic_diver_weight: float,
    lr: float,
):
    """
    æ‰“å°å½“å‰è®­ç»ƒçŠ¶æ€å¹¶ä¿å­˜åˆ°CSVæ–‡ä»¶ã€‚
    æ—¶é—´å­—ç¬¦ä¸²åœ¨å‡½æ•°å†…éƒ¨ç”Ÿæˆï¼Œæ ¼å¼ä¸º H:MM:SSï¼ˆè‹¥ >=1hï¼‰æˆ– MM:SSã€‚
    """
    import time

    # === ğŸ•’ åŠ¨æ€è®¡ç®—æ—¶é—´ ===
    current_time = time.time()
    elapsed_seconds = current_time - epoch_start_time
    steps_done = step % epoch_total_steps or epoch_total_steps  # é˜²æ­¢ step=0
    if steps_done == 0:
        steps_done = 1
    avg_time_per_step = elapsed_seconds / steps_done
    remaining_steps = epoch_total_steps - steps_done
    remaining_seconds = avg_time_per_step * max(0, remaining_steps)

    def format_hms(seconds: float) -> str:
        seconds = int(seconds)
        h = seconds // 3600
        m = (seconds % 3600) // 60
        s = seconds % 60
        if h > 0:
            return f"{h}:{m:02d}:{s:02d}"
        else:
            return f"{m:02d}:{s:02d}"

    elapsed_str = format_hms(elapsed_seconds)
    remaining_str = format_hms(remaining_seconds)

    # === ğŸ”¢ åŠ¨æ€å¯¹é½ ===
    epoch_width = len(str(total_epochs))
    step_width = len(str(total_steps))

    # === ğŸ–¨ï¸ æ‰“å°æ—¥å¿— ===
    print(
        f"[Epoch {epoch+1:>{epoch_width}}/{total_epochs} | "
        f"Step {step:>{step_width}}/{total_steps} | "
        f"{elapsed_str}<{remaining_str}] "
        f"Total: {avg_total_loss:>8.6f} | "
        f"Recon: {avg_recon_loss:>8.6f} | "
        f"Comit: {avg_comit_loss:>8.6f} | "
        f"Ortho: {avg_ortho_loss:>8.6f} | "
        f"Diver: {avg_diver_loss:>3.2f} | "
        f"Usage: {codebook_usage*100:>3.1f}% | "
        f"LR: {lr:>7.2e} |"
    )

    # === ğŸ’¾ å†™å…¥ CSV ===
    row_data = [
        epoch + 1,
        step,
        avg_recon_loss,
        avg_total_loss,
        avg_comit_loss,
        avg_diver_loss,
        avg_ortho_loss,
        codebook_usage * 100,  # ä¿å­˜ä¸ºç™¾åˆ†æ¯”æ›´ç›´è§‚ï¼ˆå¯é€‰ï¼‰
        dynamic_recon_weight,
        dynamic_comit_weight,
        dynamic_ortho_weight,
        dynamic_diver_weight,
        lr
    ]
    with open(loss_csv_path, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(row_data)

def vq_train(
    npy_dir: str,
    output_model_path: str,
    batch_size: int = 16,
    lr: float = 1e-4,
    num_epochs: int = 10,
    codebook_size: int = 8192,
    codebook_dim: int = 512,
    chunk_size: int = 12000,
    num_workers: int = 8,
    update_loss_weight_every: int = 10,
    prefetch_factor: int = 128,
    val_ratio: int = 0.1,
    do_evaluate: bool = True,
    commitment_weight: float = 1.0,
    codebook_diversity_loss_weight: float = 1.0,
    orthogonal_reg_weight: float = 1.0,
    loss_log_interval: int = 10,
    loss_csv_path: str = "train_loss.csv",  # âœ… æ–°å¢å‚æ•°ï¼šloss æ—¥å¿— CSV è·¯å¾„
    use_wandb: bool = True,                 # æ˜¯å¦å¯ç”¨ wandb
    wandb_project: str = "nanopore_vq",     # wandb é¡¹ç›®å
    wandb_name: str = "default_wandb_runname",  # è¿è¡Œåç§°ï¼ˆå¯é€‰
    # ====== ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°ï¼ˆæ–°å¢ï¼‰======
    lr_scheduler_type: str = "cosine",          # 'cosine', 'linear', 'constant'
    warmup_steps: int = 500,                    # é¢„çƒ­æ­¥æ•°ï¼ˆå…¨å±€ stepï¼‰
    warmup_start_factor: float = 1e-6,          # warmup èµ·å§‹ lr = lr * start_factor
    warmup_end_factor: float = 1.0,             # warmup ç»“æŸ lr = lr * end_factor
    main_scheduler_end_factor: float = 1e-6,    # ä¸»è°ƒåº¦å™¨æœ€ç»ˆ lr = lr * end_factorï¼ˆä»… linear ç”¨ï¼‰
    save_checkpoint_every_spoch: int = 1000,    # æ¯å¤šå°‘ä¸ªupdate_loss_weight_everyè¿›è¡Œä¸€æ¬¡æ£€æŸ¥ç‚¹ä¿å­˜
    evaluate_every_spoch: int = 1000,           # æ¯å¤šå°‘ä¸ªupdate_loss_weight_everyè¿›è¡Œä¸€æ¬¡evaluate
):
    """
    åˆ†å¸ƒå¼è®­ç»ƒ Nanopore VQ tokenizerã€‚
    ç°åœ¨ä¼šåˆ†åˆ«æ‰“å°ï¼šé‡å»ºæŸå¤±ã€commitment æŸå¤±ã€æ€»æŸå¤±ã€‚
    """
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.utils.data.distributed import DistributedSampler

    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    dist.init_process_group(backend="nccl")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_device_id = rank % torch.cuda.device_count()
    torch.cuda.set_device(local_device_id)
    device = f"cuda:{local_device_id}"

    # ========== åˆå§‹åŒ– wandbï¼ˆä»… rank 0ï¼‰==========
    if rank == 0 and use_wandb:
       import wandb
       wandb.init(
           project=wandb_project,
           name=wandb_name,
           config={
               "batch_size": batch_size,
               "lr": lr,
               "num_epochs": num_epochs,
               "codebook_size": codebook_size,
               "codebook_dim": codebook_dim,
               "chunk_size": chunk_size,
               "update_loss_weight_every": update_loss_weight_every,
               "commitment_weight": commitment_weight,
               "codebook_diversity_loss_weight": codebook_diversity_loss_weight,
               "orthogonal_reg_weight": orthogonal_reg_weight,
               "world_size": world_size,
           }
        )
    else:
        wandb = None  # é¿å…æœªå®šä¹‰


    if rank == 0:
        print(f"ğŸš€ Using {world_size} GPUs for training.")
        print(f"ğŸ“‚ Data directory: {npy_dir}")
        print(f"ğŸ’¾ Model will be saved to: {output_model_path}")
        print(f"âš™ï¸  Hyperparameters: "
              f"batch_size={batch_size}, lr={lr}, epochs={num_epochs}, "
              f"codebook_size={codebook_size}, codebook_dim={codebook_dim}, chunk_size={chunk_size}, "
              f"do_evaluate={do_evaluate}, save_checkpoint_every_spoch={save_checkpoint_every_spoch}")

        # âœ… åˆå§‹åŒ– CSV æ–‡ä»¶ï¼ˆä»… rank 0ï¼‰
        with open(loss_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            header = [
                'epoch', 'step',
                'recon_loss', 'total_loss', 'comit_loss', 'diver_loss', 'ortho_loss', 'codebook_usage',
                'wv_recon', 'wv_comit', 'wv_ortho', 'wv_diver',  # â† æ–°å¢
                'lr'
            ]
            writer.writerow(header)

    # ========== æ•°æ®åŠ è½½ ==========
    dataset = NanoporeSignalDataset(shards_dir=npy_dir)
    # ====== æ–°å¢ï¼šåªå–å‰ N ä¸ªæ ·æœ¬ï¼ˆæˆ–ä»»æ„å­é›†ï¼‰======
    #subset_size = int(1.0 * len(dataset))  # ä¾‹å¦‚ï¼šåªç”¨ 10% çš„æ•°æ®
    # æˆ–è€…æŒ‡å®šç»å¯¹æ•°é‡ï¼š
    # subset_size = 100_000
    # ç¡®ä¿ä¸è¶…é™
    #subset_size = min(subset_size, len(dataset))
    # å›ºå®šå­é›†é€‰æ‹©çš„éšæœºæ€§ï¼ˆä»…å½±å“ subset é€‰å–ï¼Œä¸å½±å“è®­ç»ƒä¸­çš„ shuffleï¼‰
    #torch.manual_seed(42)
    #indices = torch.randperm(len(dataset)).tolist()[:subset_size]
    #dataset = torch.utils.data.Subset(dataset, indices)
    # æ³¨æ„ï¼šè¿™ä¸ª seed åªæ§åˆ¶ subset é€‰å–ï¼Œä¸å½±å“ DataLoader å†…éƒ¨çš„ shuffle=True æˆ– DistributedSampler çš„æ‰“ä¹±è¡Œä¸ºã€‚


    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        pin_memory=True,
        drop_last=True
    )

    # ========== å¯é€‰ï¼šéªŒè¯é›†ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰==========
    val_loader = None
    if do_evaluate and rank == 0:  # â­ åªåœ¨ rank 0 åˆ›å»º val_loaderï¼ˆå…¶ä»– rank ä¸éœ€è¦ï¼‰
        actual_val_size = int(val_ratio *len(dataset))
        if actual_val_size < 1:
            actual_val_size = 1
        indices = np.random.choice(len(dataset), size=actual_val_size, replace=False)
        val_subset = torch.utils.data.Subset(dataset, indices)  # â† å¤ç”¨ dataset
        val_loader = DataLoader(
            val_subset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=max(2, num_workers // 2),
            pin_memory=True
        )
    # ========== æ¨¡å‹ä¸ä¼˜åŒ–å™¨ ==========
    model = NanoporeVQModel(
            codebook_size=codebook_size, 
            codebook_dim=codebook_dim, 
            commitment_weight=commitment_weight,
            codebook_diversity_loss_weight=codebook_diversity_loss_weight,
            orthogonal_reg_weight=orthogonal_reg_weight
            ).to(device)
    model = DDP(model, device_ids=[local_device_id])
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)




    # åªå¯¹å‰ä¸‰ä¸ªåšåŠ¨æ€åŠ æƒ
    if rank == 0:
        # è‡ªå®šä¹‰åˆå§‹æƒé‡ï¼ˆä¾‹å¦‚æ›´é‡è§† recon_lossï¼‰
        init_w = {
            "recon_loss": 0.25,
            "comit_loss": 0.25,
            "ortho_loss": 0.25,
            "diver_loss": 0.25
        }
        # å®šä¹‰æƒé‡è¾¹ç•Œ
        bounds = {
            "recon_loss": (0.01, 0.99),
            "comit_loss": (0.01, 0.99),
            "ortho_loss": (0.01, 0.99),
            "diver_loss": (0.01, 0.99),
        }

        dwa = DynamicWeightAverager(
            loss_names=["recon_loss", "comit_loss", "ortho_loss", "diver_loss","total_loss"],
            weighted_loss_names=["recon_loss", "comit_loss", "ortho_loss","diver_loss"],
            initial_weights=init_w,
            weight_bounds=bounds,
            warmup_steps=10,          # å‰ 200 æ­¥å›ºå®šç”¨ init_w
            temperature=1.0,
            window_size=50,
            slow_window=45,
            fast_window=5,
            device=device
        )

    # ========== å­¦ä¹ ç‡è°ƒåº¦å™¨ ==========
    if rank == 0:
        total_training_steps = len(dataloader) * num_epochs
        print(f"ğŸ”¢ Total training steps: {total_training_steps}, Warmup steps: {warmup_steps}")


    # ========== å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå®Œå…¨å‚æ•°åŒ–ï¼‰==========
    scheduler = None
    total_training_steps = len(dataloader) * num_epochs

    if rank == 0:
        print(f"ğŸ”¢ Total training steps: {total_training_steps}")
        if lr_scheduler_type != "constant":
            print(f"ğŸ“ˆ Using LR scheduler: {lr_scheduler_type}, "
                  f"warmup_steps={warmup_steps}, "
                  f"warmup: {warmup_start_factor}â†’{warmup_end_factor}, "
                  f"main_end_factor={main_scheduler_end_factor}")

    if lr_scheduler_type != "constant":
        from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR

        # Warmup é˜¶æ®µï¼šä» warmup_start_factor * lr åˆ° warmup_end_factor * lr
        warmup_scheduler = LinearLR(
            optimizer,
            start_factor=warmup_start_factor,
            end_factor=warmup_end_factor,
            total_iters=warmup_steps
        )

        main_steps = max(1, total_training_steps - warmup_steps)

        if lr_scheduler_type == "cosine":
            # Cosine é€€ç«ï¼šä»å½“å‰ lrï¼ˆå³ warmup_end_factor * lrï¼‰é€€ç«åˆ° 0
            main_scheduler = CosineAnnealingLR(optimizer, T_max=main_steps)
        elif lr_scheduler_type == "linear":
            # Linear è¡°å‡ï¼šä»å½“å‰ lr è¡°å‡åˆ° main_scheduler_end_factor * åŸå§‹ lr
            # æ³¨æ„ï¼šLinearLR çš„ end_factor æ˜¯ç›¸å¯¹äº warmup ç»“æŸæ—¶çš„ lr
            # æ‰€ä»¥ç›®æ ‡ lr = (main_scheduler_end_factor * lr) / (warmup_end_factor * lr) = main_scheduler_end_factor / warmup_end_factor
            relative_end_factor = main_scheduler_end_factor / warmup_end_factor if warmup_end_factor > 0 else 0.0
            relative_end_factor = max(1e-8, min(1.0, relative_end_factor))  # å®‰å…¨ clamp
            main_scheduler = LinearLR(
                optimizer,
                start_factor=1.0,
                end_factor=relative_end_factor,
                total_iters=main_steps
            )
        else:
            raise ValueError(f"Unsupported lr_scheduler_type: {lr_scheduler_type}")

        scheduler = SequentialLR(
            optimizer,
            schedulers=[warmup_scheduler, main_scheduler],
            milestones=[warmup_steps]
        )
    # else: scheduler remains None â†’ constant LR

    # ========== è¯„ä¼°å‡½æ•°ï¼ˆä»…åœ¨ do_evaluate=True æ—¶è°ƒç”¨ï¼‰==========

    def evaluate_codebook_usage():
        if val_loader is None:  # â­ å®‰å…¨æ£€æŸ¥
            return 0.0, 0
        model.eval()
        used_codes = set()
        total_tokens = 0
        with torch.no_grad():
            for batch in val_loader:
                x = batch.to(device)
                _, indices, _, _ = model.module(x)
                indices = indices.cpu().numpy().flatten()
                used_codes.update(indices.tolist())
                total_tokens += indices.size
        usage_ratio = len(used_codes) / codebook_size
        model.train()
        return usage_ratio, total_tokens
    # ========== è®­ç»ƒå¾ªç¯ ==========
    model.train()
    codebook_usage = 0.0
    total_steps = len(dataloader)*num_epochs
    epoch_total_steps = len(dataloader)  # å½“å‰ epoch çš„æœ¬åœ° step æ•°ï¼ˆæ¯ä¸ª rank ç›¸åŒï¼‰
    # ğŸ‘‡ æ–°å¢ï¼šç¼“å­˜æƒé‡ï¼ˆåˆå§‹å€¼å¯è®¾ä¸º 1.0ï¼‰
    cached_wvalue = torch.tensor([0.25, 0.25, 0.25,0.25], device=device)  # [recon, comit, ortho]
    # åœ¨ for epoch in range(num_epochs): ä¹‹å‰
    loss_buffer = {
        "recon": [],
        "comit": [],
        "ortho": [],
        "diver": []
    }
    # æ¯10ä¸ªstepå°±æ˜¯ä¸€ä¸ªspoch
    spoch = 0
    total_spochs = int(total_steps/update_loss_weight_every)
    for epoch in range(num_epochs):
        epoch_start_time = time.time()  # â† æ–°å¢ï¼šè®°å½• epoch å¼€å§‹æ—¶é—´
        sampler.set_epoch(epoch)
        num_batches = torch.tensor(len(dataloader), device=device)
        for step, batch in enumerate(dataloader):
            x = batch.to(device)
            # break_loss æ˜¯å¦å·²åŒ…å« commitment_weightï¼Ÿ
            # åœ¨ vector_quantize_pytorch ä¸­ï¼Œè¿”å›çš„ break_loss å·²ç»æ˜¯ä¹˜è¿‡ commitment_weight çš„ï¼ˆé»˜è®¤ 0.25ï¼‰
            # å› ä¸º VectorQuantize è¿”å›çš„ break_loss æ˜¯ï¼š
            # break_loss = (z_e - e_k.detach()).pow(2).mean() * self.commitment_weight
            # å®ƒæ˜¯ä¸€ä¸ª requires_grad=False çš„ scalar tensorï¼Œä½äºä¸è¾“å…¥ç›¸åŒçš„è®¾å¤‡ä¸Šï¼ˆGPUï¼‰ã€‚
            # æ‰€ä»¥ break_loss æœ¬èº«å°±æ˜¯ GPU tensorï¼Œä¸éœ€è¦ .item()ã€‚
            recon, indices,break_loss, loss_breakdown = model(x)
            # å¦‚æœä½ æƒ³å¼±åŒ–é‡å»ºã€å¼ºè°ƒç¦»æ•£è¡¨ç¤ºè´¨é‡ï¼Œå¯ä»¥åŠ ä¸€ä¸ªè¶…å‚æ•°ï¼š
            # recon_weight = 0.01  # << é™ä½é‡å»ºæƒé‡
            # loss = recon_weight * F.mse_loss(recon, x) + break_loss
            # è¿™æ ·æ¨¡å‹ä¼šæ›´å…³æ³¨â€œç¼–ç å™¨è´´ç´§ç æœ¬â€å’Œâ€œç æœ¬åˆ†æ•£â€ï¼Œè€Œä¸æ˜¯åƒç´ çº§è¿˜åŸä¿¡å·â€”â€”éå¸¸é€‚åˆåš tokenizerã€‚
            recon_loss = F.mse_loss(recon, x)
            comit_loss = loss_breakdown.commitment
            diver_loss = loss_breakdown.codebook_diversity
            ortho_loss = loss_breakdown.orthogonal_reg
            #print("comit_loss grad:", comit_loss.requires_grad) # True
            total_loss = (recon_loss + 
                comit_loss * (commitment_weight+epoch) + 
                ortho_loss * orthogonal_reg_weight + 
                diver_loss * codebook_diversity_loss_weight)
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            # ğŸ‘‡ æ›´æ–°å­¦ä¹ ç‡ï¼ˆæ¯ä¸ª stepï¼‰
            if scheduler is not None:
                scheduler.step()
            # ğŸ‘‡ åªç¼“å­˜æ ‡é‡å€¼ï¼ˆæ— æ¢¯åº¦ï¼‰
            loss_buffer["recon"].append(recon_loss.item())
            loss_buffer["comit"].append(comit_loss.item())
            loss_buffer["ortho"].append(ortho_loss.item())
            loss_buffer["diver"].append(diver_loss.item())
            # ====== ğŸ” åŠ¨æ€æƒé‡æ›´æ–°é€»è¾‘ï¼ˆæ¯éš” update_every æ­¥ï¼‰ ======
            wv_recon, wv_comit, wv_ortho,wv_diver = cached_wvalue.tolist()
            should_update_weights = (step + 1) % update_loss_weight_every == 0 or  (step == len(dataloader) - 1)
            if should_update_weights:
                spoch += 1
                # è®¡ç®—å½“å‰çª—å£å¹³å‡ï¼ˆé˜²æ­¢ç©ºï¼‰
                def safe_mean(lst):
                    return sum(lst) / len(lst) if lst else 0.0
                local_avg_losses = torch.tensor([
                    safe_mean(loss_buffer["recon"]),
                    safe_mean(loss_buffer["comit"]),
                    safe_mean(loss_buffer["ortho"]),
                    safe_mean(loss_buffer["diver"])
                ], device=device)
                # ğŸ‘‡ å…¨å±€åŒæ­¥ï¼šæ±‚æ‰€æœ‰ rank çš„å¹³å‡
                # â† æ‰€æœ‰ rank åœ¨è¿™é‡ŒåŒæ­¥ï¼Œloss å·²å¹³å‡ æœ¬èº«å°±èµ·åˆ°äº† éšå¼çš„ barrier ä½œç”¨ï¼Œæ— éœ€å†æ‰‹åŠ¨åŠ  dist.barri
                dist.all_reduce(local_avg_losses, op=dist.ReduceOp.AVG)
                global_avg_recon, global_avg_comit, global_avg_ortho, global_avg_diver = local_avg_losses.tolist()
                global_avg_total = (
                            global_avg_recon +
                            global_avg_comit * commitment_weight +
                            global_avg_ortho * orthogonal_reg_weight +
                            global_avg_diver * codebook_diversity_loss_weight )

                if rank == 0:
                    current_losses = {
                        "recon_loss": global_avg_recon,
                        "comit_loss": global_avg_comit,
                        "ortho_loss": global_avg_ortho,
                        "diver_loss": global_avg_diver,
                        "total_loss": global_avg_total
                    }
                    wvalue = dwa.update_and_get_weights(current_losses)
                    wvalue_tensor = torch.tensor([
                        wvalue["recon_loss"],
                        wvalue["comit_loss"],
                        wvalue["ortho_loss"],
                        wvalue["diver_loss"],
                    ], device=device)
                else:
                    wvalue_tensor = torch.empty(4, device=device)
                # å¹¿æ’­æ–°æƒé‡
                dist.broadcast(wvalue_tensor, src=0) # â† æ‰€æœ‰ rank åœ¨è¿™é‡ŒåŒæ­¥ï¼Œæ”¶åˆ°å¹¿æ’­çš„æƒé‡  æœ¬èº«å°±èµ·åˆ°äº† éšå¼çš„ barrier ä½œç”¨ï¼Œæ— éœ€å†æ‰‹åŠ¨åŠ  dist.barrier()ã€‚
                cached_wvalue = wvalue_tensor  # æ›´æ–°ç¼“å­˜
                # ğŸ” æ¸…ç©º bufferï¼Œä¸ºä¸‹ä¸€ä¸ªçª—å£å‡†å¤‡
                loss_buffer = {k: [] for k in loss_buffer}
                    

                if rank == 0:
                    current_lr = optimizer.param_groups[0]['lr']
                    # è·å–æœ€æ–° fast lossï¼ˆå¯ç”¨äºæ—¥å¿—ã€è°ƒè¯•ã€ç›‘æ§ï¼‰
                    global_step = epoch * len(dataloader) + (step + 1)
                    log_and_save(
                        epoch=epoch,
                        step=global_step,
                        total_epochs=num_epochs,
                        total_steps=total_steps,
                        epoch_start_time=epoch_start_time,      # âœ… ä¼ å…¥æ—¶é—´æˆ³
                        epoch_total_steps=len(dataloader),      # âœ… ç”¨äºä¼°ç®—å‰©ä½™æ—¶é—´
                        avg_recon_loss=global_avg_recon,
                        avg_total_loss=global_avg_total,
                        avg_comit_loss=global_avg_comit,
                        avg_diver_loss=global_avg_diver,
                        avg_ortho_loss=global_avg_ortho,
                        codebook_usage=codebook_usage,
                        loss_csv_path=loss_csv_path,
                        dynamic_recon_weight=wv_recon,
                        dynamic_comit_weight=wv_comit,
                        dynamic_ortho_weight=wv_ortho,
                        dynamic_diver_weight=wv_diver,
                        lr=current_lr
                    )
                    # === ğŸ“Š wandb æ—¥å¿— ===
                    log_dict = {
                        "train/recon_loss": global_avg_recon,
                        "train/comit_loss": global_avg_comit,
                        "train/ortho_loss": global_avg_ortho,
                        "train/diver_loss": global_avg_diver,
                        "train/total_loss": global_avg_total,
                        "train/codebook_usage": codebook_usage,
                        "weights/recon": wv_recon,
                        "weights/comit": wv_comit,
                        "weights/ortho": wv_ortho,
                        "weights/diver": wv_diver,
                        "epoch": epoch + 1,
                        "learning_rate": current_lr,  # å¦‚æœä½¿ç”¨ schedulerï¼Œå¯åŠ¨æ€è·å–
                    }
                    if use_wandb:
                        wandb.log(log_dict, step=global_step)

                if rank == 0 and (spoch + 1)% evaluate_every_spoch == 0 and spoch < total_spochs:
                    codebook_usage, total_tokens = evaluate_codebook_usage()
                    print(
                        f"Spoch {spoch+1} - "
                        f"Codebook Usage: {codebook_usage:.2%} "
                        )
                if rank == 0 and (spoch + 1)% save_checkpoint_every_spoch == 0:
                    # âœ… æ£€æŸ¥ç‚¹ä¿å­˜é€»è¾‘ï¼ˆä»… rank 0ï¼‰
                    checkpoint_path = f"{output_model_path}.spoch{spoch+1}.pth"
                    torch.save(model.module.state_dict(), checkpoint_path)
                    print(f"âœ… Checkpoint saved to {checkpoint_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆä»… rank 0ï¼‰
    if rank == 0:
        torch.save(model.module.state_dict(), output_model_path)
        print(f"âœ… Final model saved to {output_model_path}")
        if use_wandb:
            wandb.finish()  # âœ… æ­£ç¡®å…³é—­
    dist.barrier()
    dist.destroy_process_group()
