# nanopore_signal_tokenizer/fast5.py

import warnings
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*")

import os
import numpy as np
import glob
from ont_fast5_api.fast5_interface import get_fast5_file
from .nanopore import nanopore_normalize, nanopore_filter
from scipy.signal import medfilt
from pathos.multiprocessing import ProcessPool
from multiprocessing import cpu_count


class Fast5Dir:
    """
    å°† Nanopore åŸå§‹ .fast5 æ–‡ä»¶æ‰¹é‡è½¬æ¢ä¸ºé¢„å¤„ç†åçš„ chunked .npy æ–‡ä»¶ã€‚

    ğŸ“Œ ä¿¡å·å¤„ç†æµæ°´çº¿ï¼ˆæ‰€æœ‰æ­¥éª¤åœ¨ to_chunks_parallel ä¸­æ§åˆ¶ï¼‰ï¼š
        1. ã€ç¼©æ”¾ã€‘raw â†’ pAï¼›
        2. ã€å½’ä¸€åŒ–ã€‘median-MADï¼ˆå¯é€‰ï¼‰ï¼›
        3. ã€ä¸­å€¼æ»¤æ³¢ã€‘kernel=5ï¼ˆå¯é€‰ï¼‰ï¼›
        4. ã€ä½é€šæ»¤æ³¢ã€‘Butterworthï¼ˆå¯é€‰ï¼‰ï¼›
        5. ã€åˆ†å—ã€‘æ»‘åŠ¨çª—å£ã€‚

    ğŸ“¦ è¾“å‡ºï¼šæ¯ä¸ª .fast5 â†’ ä¸€ä¸ª .npyï¼Œå†…å®¹ä¸º list[dict]ï¼Œå« read_idã€ä½ç½®ã€chunk_dataã€‚
    """

    def __init__(self, fast5_dir: str, default_fs: int = 5000):
        """
        åˆå§‹åŒ–ç›®å½•å¤„ç†å™¨ã€‚

        Args:
            fast5_dir (str): åŒ…å« .fast5 æ–‡ä»¶çš„ç›®å½•ã€‚
            default_fs (int): é»˜è®¤é‡‡æ ·ç‡ï¼ˆHzï¼‰ï¼Œç”¨äºç¼ºå¤± metadata çš„æƒ…å†µã€‚
        """
        if not os.path.isdir(fast5_dir):
            raise ValueError(f"FAST5 directory does not exist: {fast5_dir}")

        self.fast5_dir = fast5_dir
        self.fast5_files = sorted(glob.glob(os.path.join(fast5_dir, "*.fast5")))
        self.default_fs = default_fs

        if not self.fast5_files:
            raise FileNotFoundError(f"No .fast5 files found in {fast5_dir}")

    @staticmethod
    def get_sampling_rate_from_read(read):
        try:
            channel_info = read.handle[read.global_key + 'channel_id'].attrs
            return int(channel_info['sampling_rate'])
        except Exception:
            return None

    def _sliding_window_chunks_with_pos(self, signal: np.ndarray, window_size: int, stride: int):
        n_points = len(signal)
        if n_points < window_size:
            return []

        chunks = []
        start = 0
        while start + window_size <= n_points:
            end = start + window_size
            chunks.append({
                'chunk_start': start,
                'chunk_end': end,
                'chunk_data': signal[start:end].copy()
            })
            start += stride
        return chunks

    def _process_single_fast5(
        self,
        fast5_path: str,
        output_dir: str,
        window_size: int,
        stride: int,
        do_normalize: bool,
        do_medianfilter: bool,
        do_lowpassfilter: bool,
    ):
        """å¤„ç†å•ä¸ª FAST5 æ–‡ä»¶ï¼Œä½¿ç”¨ä¼ å…¥çš„å¤„ç†é€‰é¡¹ã€‚"""
        all_chunks = []
        try:
            with get_fast5_file(fast5_path, mode="r") as f5:
                for read in f5.get_reads():
                    # Step 1: raw â†’ pA
                    channel_info = read.handle[read.global_key + 'channel_id'].attrs
                    offset = int(channel_info['offset'])
                    scaling = channel_info['range'] / channel_info['digitisation']
                    raw = read.handle[read.raw_dataset_name][:]
                    signal = np.array(scaling * (raw + offset), dtype=np.float32)

                    # Step 2: normalize
                    if do_normalize:
                        signal = nanopore_normalize(signal)
                    if signal.size == 0 or np.isnan(signal).any():
                        print(f"âš ï¸ Invalid signal after normalization for read {read.read_id}, skipped.")
                        continue

                    # Step 3: median filter
                    if do_medianfilter:
                        signal = medfilt(signal, kernel_size=5).astype(np.float32)

                    # Step 4: low-pass filter
                    if do_lowpassfilter:
                        fs_from_read = self.get_sampling_rate_from_read(read)
                        fs = fs_from_read if fs_from_read is not None else self.default_fs
                        try:
                            filtered_signal = nanopore_filter(signal, fs=fs)
                        except Exception as e:
                            print(f"âš ï¸ Filtering failed for read {read.read_id} (fs={fs}): {e}, skipped.")
                            continue
                        if filtered_signal.size == 0 or np.isnan(filtered_signal).any():
                            print(f"âš ï¸ Invalid signal after filtering for read {read.read_id}, skipped.")
                            continue
                        signal = filtered_signal

                    # Step 5: chunking
                    chunks = self._sliding_window_chunks_with_pos(signal, window_size, stride)
                    if not chunks:
                        print(f"âš ï¸ Read {read.read_id} too short (<{window_size} points), skipped.")
                        continue

                    for ch in chunks:
                        all_chunks.append({
                            'read_id': read.read_id,
                            'chunk_start_pos': ch['chunk_start'],
                            'chunk_end_pos': ch['chunk_end'],
                            'chunk_data': ch['chunk_data']
                        })

            # Save
            if all_chunks:
                basename = os.path.basename(fast5_path).rsplit('.', 1)[0]
                save_path = os.path.join(output_dir, f"{basename}.npy")
                np.save(save_path, all_chunks)
                print(f"âœ… Saved {len(all_chunks)} chunks from {basename} to {save_path}")
            else:
                print(f"âš ï¸ No valid chunks in {os.path.basename(fast5_path)}, skipping save.")

        except Exception as e:
            print(f"âŒ Critical error processing {fast5_path}: {e}")

    def to_chunks(
        self,
        output_dir: str,
        window_size: int = 32,
        stride: int = 8,
        do_normalize: bool = True,
        do_medianfilter: bool = False,
        do_lowpassfilter: bool = False,
        n_jobs: int = -1,
    ):
        """
        å¹¶è¡Œå¤„ç†æ•´ä¸ª FAST5 ç›®å½•ï¼Œç”Ÿæˆ chunked .npy æ–‡ä»¶ã€‚

        Args:
            output_dir (str): è¾“å‡ºç›®å½•ã€‚
            window_size (int): æ¯ä¸ª chunk çš„é•¿åº¦ï¼ˆé»˜è®¤ 32ï¼‰ã€‚
            stride (int): æ»‘åŠ¨æ­¥é•¿ï¼ˆé»˜è®¤ 8ï¼‰ã€‚
            do_normalize (bool): æ˜¯å¦æ‰§è¡Œ median-MAD å½’ä¸€åŒ–ã€‚
            do_medianfilter (bool): æ˜¯å¦åº”ç”¨ä¸­å€¼æ»¤æ³¢ã€‚
            do_lowpassfilter (bool): æ˜¯å¦åº”ç”¨ä½é€šæ»¤æ³¢ã€‚
            n_jobs (int): å¹¶è¡Œè¿›ç¨‹æ•°ã€‚-1 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ CPU æ ¸å¿ƒã€‚
        """
        os.makedirs(output_dir, exist_ok=True)

        if n_jobs == -1:
            n_jobs = cpu_count()

        print(f"ğŸ“ Processing {len(self.fast5_files)} FAST5 files from: {self.fast5_dir}")
        print(f"ParallelGroup: using {n_jobs} processes")
        print(f"âš™ï¸  Signal pipeline:")
        print(f"    - Normalize: {'ON' if do_normalize else 'OFF'}")
        print(f"    - Median Filter: {'ON' if do_medianfilter else 'OFF'}")
        print(f"    - Low-pass Filter: {'ON' if do_lowpassfilter else 'OFF'}")
        print(f"ğŸ’¾ Saving chunks to: {output_dir}")

        # æ„é€ å‚æ•°åˆ—è¡¨
        args_list = [
            (
                fp,
                output_dir,
                window_size,
                stride,
                do_normalize,
                do_medianfilter,
                do_lowpassfilter,
            )
            for fp in self.fast5_files
        ]

        # ä½¿ç”¨ pathos å¹¶è¡Œå¤„ç†
        with ProcessPool(nodes=n_jobs) as pool:
            pool.map(self._process_single_fast5_wrapper, args_list)

    def _process_single_fast5_wrapper(self, args):
        """ä¾› pathos è°ƒç”¨çš„åŒ…è£…å™¨ã€‚"""
        (
            fast5_path,
            output_dir,
            window_size,
            stride,
            do_normalize,
            do_medianfilter,
            do_lowpassfilter,
        ) = args
        return self._process_single_fast5(
            fast5_path=fast5_path,
            output_dir=output_dir,
            window_size=window_size,
            stride=stride,
            do_normalize=do_normalize,
            do_medianfilter=do_medianfilter,
            do_lowpassfilter=do_lowpassfilter,
        )
