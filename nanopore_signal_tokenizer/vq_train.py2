import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import csv  # âœ… æ–°å¢ï¼šç”¨äºå†™å…¥ CSV
import time  # ç¡®ä¿å·²å¯¼å…¥
# ç›¸å¯¹å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from .dataset import NanoporeSignalDataset
from .vq_model import NanoporeVQModel
from typing import Dict, List
import collections
from .dwa import DynamicWeightAverager 
def log_and_save(epoch, step, total_steps, elapsed_time, remaining_time, avg_recon_loss, avg_break_loss,
                 avg_total_loss, avg_comit_loss, avg_diver_loss, avg_ortho_loss, codebook_usage, 
                 loss_csv_path, rank=0):
    """
    æ‰“å°å½“å‰è®­ç»ƒçŠ¶æ€å¹¶ä¿å­˜åˆ°CSVæ–‡ä»¶ã€‚
    
    å‚æ•°ï¼š
        epoch (int): å½“å‰epochæ•°ã€‚
        step (int): å½“å‰æ­¥éª¤æ•°ã€‚
        total_steps (int): æ€»æ­¥éª¤æ•°ã€‚
        elapsed_time (str): å·²ç»è¿‡çš„æ—¶é—´ï¼Œæ ¼å¼ä¸º"MM:SS"ã€‚
        remaining_time (str): é¢„è®¡å‰©ä½™æ—¶é—´ï¼Œæ ¼å¼ä¸º"MM:SS"ã€‚
        avg_recon_loss (float): å¹³å‡é‡å»ºæŸå¤±ã€‚
        avg_break_loss (float): å¹³å‡æ–­è¨€æŸå¤±ã€‚
        avg_total_loss (float): å¹³å‡æ€»æŸå¤±ã€‚
        avg_comit_loss (float): å¹³å‡commitmentæŸå¤±ã€‚
        avg_diver_loss (float): å¹³å‡å¤šæ ·æ€§æŸå¤±ã€‚
        avg_ortho_loss (float): å¹³å‡æ­£äº¤æ€§æŸå¤±ã€‚
        codebook_usage (float): codebookä½¿ç”¨ç‡ã€‚
        loss_csv_path (str): ä¿å­˜æŸå¤±æ•°æ®çš„CSVæ–‡ä»¶è·¯å¾„ã€‚
        rank (int, optional): è¿›ç¨‹æ’åï¼Œé»˜è®¤ä¸º0ã€‚
    """
    if rank == 0:
        dynamic_recon_weight = 0.5  # ç¤ºä¾‹æƒé‡ï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        dynamic_comit_weight = 0.3
        dynamic_diver_weight = 0.0
        dynamic_ortho_weight = 0.2
        
        print(
            f"[Epoch {epoch+1}/{total_steps} | Step {step+1}/{total_steps} | "
            f"{elapsed_time}<{remaining_time}] "
            f"Total: {avg_total_loss:.6f} | "
            f"Recon: {avg_recon_loss:.6f} * {dynamic_recon_weight:.3f} | "
            f"Comit: {avg_comit_loss:.6f} * {dynamic_comit_weight:.3f} | "
            f"Diver: {avg_diver_loss:.6f} * {dynamic_diver_weight:.3f} | "
            f"Ortho: {avg_ortho_loss:.6f} * {dynamic_ortho_weight:.3f} | "
            f"Usage: {codebook_usage:.6f} | "
        )
        
        row_data = [
            epoch + 1,
            step + 1,
            avg_recon_loss,
            avg_break_loss,
            avg_total_loss,
            avg_comit_loss,
            avg_diver_loss,
            avg_ortho_loss,
            codebook_usage
        ]
        with open(loss_csv_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(row_data)

def vq_train(
    npy_dir: str,
    output_model_path: str,
    batch_size: int = 16,
    lr: float = 3e-4,
    num_epochs: int = 10,
    codebook_size: int = 8192,
    codebook_dim: int = 512,
    chunk_size: int = 12000,
    num_workers: int = 8,
    prefetch_factor: int = 128,
    val_ratio: int = 0.1,
    do_evaluate: bool = True,
    commitment_weight: float = 1.0,
    codebook_diversity_loss_weight: float = 1.0,
    orthogonal_reg_weight: float = 1.0,
    loss_log_interval: int = 10,
    loss_csv_path: str = "train_loss.csv",  # âœ… æ–°å¢å‚æ•°ï¼šloss æ—¥å¿— CSV è·¯å¾„
    save_checkpoint_interval: int = 10         # âœ… æ–°å¢å‚æ•°ï¼šæ¯éš” N ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
):
    """
    åˆ†å¸ƒå¼è®­ç»ƒ Nanopore VQ tokenizerã€‚
    ç°åœ¨ä¼šåˆ†åˆ«æ‰“å°ï¼šé‡å»ºæŸå¤±ã€commitment æŸå¤±ã€æ€»æŸå¤±ã€‚
    """
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.utils.data.distributed import DistributedSampler

    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    dist.init_process_group(backend="nccl")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_device_id = rank % torch.cuda.device_count()
    torch.cuda.set_device(local_device_id)
    device = f"cuda:{local_device_id}"

    if rank == 0:
        print(f"ğŸš€ Using {world_size} GPUs for training.")
        print(f"ğŸ“‚ Data directory: {npy_dir}")
        print(f"ğŸ’¾ Model will be saved to: {output_model_path}")
        print(f"âš™ï¸  Hyperparameters: "
              f"batch_size={batch_size}, lr={lr}, epochs={num_epochs}, "
              f"codebook_size={codebook_size}, codebook_dim={codebook_dim}, chunk_size={chunk_size}, "
              f"do_evaluate={do_evaluate}, save_checkpoint_interval={save_checkpoint_interval}")

        # âœ… åˆå§‹åŒ– CSV æ–‡ä»¶ï¼ˆä»… rank 0ï¼‰
        with open(loss_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            header = [
                'epoch',
                'step',
                'recon_loss',
                'break_loss',
                'total_loss',
                'comit_loss',
                'diver_loss',
                'ortho_loss',
                'codebook_usage',
            ]
            writer.writerow(header)

    # ========== æ•°æ®åŠ è½½ ==========
    dataset = NanoporeSignalDataset(shards_dir=npy_dir)
    # ====== æ–°å¢ï¼šåªå–å‰ N ä¸ªæ ·æœ¬ï¼ˆæˆ–ä»»æ„å­é›†ï¼‰======
    subset_size = int(0.3 * len(dataset))  # ä¾‹å¦‚ï¼šåªç”¨ 10% çš„æ•°æ®
    # æˆ–è€…æŒ‡å®šç»å¯¹æ•°é‡ï¼š
    # subset_size = 100_000
    # ç¡®ä¿ä¸è¶…é™
    subset_size = min(subset_size, len(dataset))
    # å›ºå®šå­é›†é€‰æ‹©çš„éšæœºæ€§ï¼ˆä»…å½±å“ subset é€‰å–ï¼Œä¸å½±å“è®­ç»ƒä¸­çš„ shuffleï¼‰
    torch.manual_seed(42)
    indices = torch.randperm(len(dataset)).tolist()[:subset_size]
    dataset = torch.utils.data.Subset(dataset, indices)
    # æ³¨æ„ï¼šè¿™ä¸ª seed åªæ§åˆ¶ subset é€‰å–ï¼Œä¸å½±å“ DataLoader å†…éƒ¨çš„ shuffle=True æˆ– DistributedSampler çš„æ‰“ä¹±è¡Œä¸ºã€‚


    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        pin_memory=True,
        drop_last=True
    )

    # ========== å¯é€‰ï¼šéªŒè¯é›†ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰==========
    val_loader = None
    if do_evaluate and rank == 0:  # â­ åªåœ¨ rank 0 åˆ›å»º val_loaderï¼ˆå…¶ä»– rank ä¸éœ€è¦ï¼‰
        actual_val_size = int(val_ratio *len(dataset))
        if actual_val_size < 1:
            actual_val_size = 1
        indices = np.random.choice(len(dataset), size=actual_val_size, replace=False)
        val_subset = torch.utils.data.Subset(dataset, indices)  # â† å¤ç”¨ dataset
        val_loader = DataLoader(
            val_subset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=max(2, num_workers // 2),
            pin_memory=True
        )
    # ========== æ¨¡å‹ä¸ä¼˜åŒ–å™¨ ==========
    model = NanoporeVQModel(
            codebook_size=codebook_size, 
            codebook_dim=codebook_dim, 
            commitment_weight=commitment_weight,
            codebook_diversity_loss_weight=codebook_diversity_loss_weight,
            orthogonal_reg_weight=orthogonal_reg_weight
            ).to(device)
    model = DDP(model, device_ids=[local_device_id])
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    # åˆå§‹åŒ– DWA
    dwa = DynamicWeightAverager(
        loss_names=["recon_loss", "comit_loss", "ortho_loss"],
        update_every=10,
        window_size=15,
        temperature=1.0,
        min_weight=0.1,
        device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
    )

    # ========== è¯„ä¼°å‡½æ•°ï¼ˆä»…åœ¨ do_evaluate=True æ—¶è°ƒç”¨ï¼‰==========
    def evaluate_codebook_usage():
        if val_loader is None:  # â­ å®‰å…¨æ£€æŸ¥
            return 0.0, 0
        model.eval()
        used_codes = set()
        total_tokens = 0
        with torch.no_grad():
            for batch in val_loader:
                x = batch.to(device)
                _, indices, _, _ = model(x)
                indices = indices.cpu().numpy().flatten()
                used_codes.update(indices.tolist())
                total_tokens += indices.size
        usage_ratio = len(used_codes) / codebook_size
        model.train()
        return usage_ratio, total_tokens
    # ========== è®­ç»ƒå¾ªç¯ ==========
    model.train()
    codebook_usage = 0.0
    total_steps = len(dataloader)*num_epochs
    for epoch in range(num_epochs):
        epoch_start_time = time.time()  # â† æ–°å¢ï¼šè®°å½• epoch å¼€å§‹æ—¶é—´
        # åˆ†åˆ«è®°å½•ä¸‰ç§æŸå¤±
        total_recon_loss = torch.tensor(0.0, device=device)
        total_break_loss = torch.tensor(0.0, device=device)
        total_total_loss = torch.tensor(0.0, device=device)
        total_comit_loss = torch.tensor(0.0, device=device)
        total_diver_loss = torch.tensor(0.0, device=device)
        total_ortho_loss = torch.tensor(0.0, device=device)

        # åˆ†åˆ«è®°å½•ä¸‰ç§æŸå¤±
        step_recon_loss = torch.tensor(0.0, device=device)
        step_break_loss = torch.tensor(0.0, device=device)
        step_total_loss = torch.tensor(0.0, device=device)
        step_comit_loss = torch.tensor(0.0, device=device)
        step_diver_loss = torch.tensor(0.0, device=device)
        step_ortho_loss = torch.tensor(0.0, device=device)

        sampler.set_epoch(epoch)
        num_batches = torch.tensor(len(dataloader), device=device)
        for step, batch in enumerate(dataloader):
            x = batch.to(device)
            # break_loss æ˜¯å¦å·²åŒ…å« commitment_weightï¼Ÿ
            # åœ¨ vector_quantize_pytorch ä¸­ï¼Œè¿”å›çš„ break_loss å·²ç»æ˜¯ä¹˜è¿‡ commitment_weight çš„ï¼ˆé»˜è®¤ 0.25ï¼‰
            # å› ä¸º VectorQuantize è¿”å›çš„ break_loss æ˜¯ï¼š
            # break_loss = (z_e - e_k.detach()).pow(2).mean() * self.commitment_weight
            # å®ƒæ˜¯ä¸€ä¸ª requires_grad=False çš„ scalar tensorï¼Œä½äºä¸è¾“å…¥ç›¸åŒçš„è®¾å¤‡ä¸Šï¼ˆGPUï¼‰ã€‚
            # æ‰€ä»¥ break_loss æœ¬èº«å°±æ˜¯ GPU tensorï¼Œä¸éœ€è¦ .item()ã€‚
            recon, indices,break_loss, loss_breakdown = model(x)
            # å¦‚æœä½ æƒ³å¼±åŒ–é‡å»ºã€å¼ºè°ƒç¦»æ•£è¡¨ç¤ºè´¨é‡ï¼Œå¯ä»¥åŠ ä¸€ä¸ªè¶…å‚æ•°ï¼š
            # recon_weight = 0.01  # << é™ä½é‡å»ºæƒé‡
            # loss = recon_weight * F.mse_loss(recon, x) + break_loss
            # è¿™æ ·æ¨¡å‹ä¼šæ›´å…³æ³¨â€œç¼–ç å™¨è´´ç´§ç æœ¬â€å’Œâ€œç æœ¬åˆ†æ•£â€ï¼Œè€Œä¸æ˜¯åƒç´ çº§è¿˜åŸä¿¡å·â€”â€”éå¸¸é€‚åˆåš tokenizerã€‚
            recon_loss = F.mse_loss(recon, x)
            #comit_loss = loss_breakdown.commitment*commitment_weight
            #diver_loss = loss_breakdown.codebook_diversity*codebook_diversity_loss_weight
            #ortho_loss = loss_breakdown.orthogonal_reg*orthogonal_reg_weight
            comit_loss = loss_breakdown.commitment
            diver_loss = loss_breakdown.codebook_diversity
            ortho_loss = loss_breakdown.orthogonal_reg
            # ç”¨ï¼ˆå¯èƒ½åˆšæ›´æ–°çš„ï¼‰æƒé‡è®¡ç®— total_loss
            current_losses = {
                "recon_loss": recon_loss.item(),
                "comit_loss": comit_loss.item(),
                "ortho_loss": ortho_loss.item()
            }
            # è·å–å½“å‰åŠ¨æ€æƒé‡
            #weights = dwa.update_and_get_weights(current_losses)
            weights = {"recon_loss":0.5,"comit_loss":0.3,"ortho_loss":0.2}
            # åŠ æƒæ€» loss
            total_loss = (
                weights["recon_loss"] * recon_loss +
                weights["comit_loss"] * comit_loss +
                weights["ortho_loss"] * ortho_loss
            )


            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
    

            # ç´¯åŠ å„éƒ¨åˆ†æŸå¤±ï¼ˆæ³¨æ„ï¼šbreak_loss æ˜¯æ ‡é‡ tensorï¼‰
            total_recon_loss += recon_loss
            total_break_loss += break_loss
            total_total_loss += total_loss
            total_comit_loss += comit_loss
            total_diver_loss += diver_loss
            total_ortho_loss += ortho_loss

            step_recon_loss += recon_loss
            step_break_loss += break_loss
            step_total_loss += total_loss
            step_comit_loss += comit_loss
            step_diver_loss += diver_loss
            step_ortho_loss += ortho_loss

            if (step + 1) % loss_log_interval == 0 or step == len(dataloader) - 1:
               
                # === ğŸ•’ æ—¶é—´ä¼°ç®— ===
                elapsed = time.time() - epoch_start_time
                steps_done = step + 1
                avg_time_per_step = elapsed / steps_done
                remaining_steps = total_steps - steps_done
                estimated_remaining = avg_time_per_step * remaining_steps
                def format_time(seconds):
                    m = int(seconds) // 60
                    s = int(seconds) % 60
                    return f"{m:02d}:{s:02d}"
                elapsed_str = format_time(elapsed)
                remaining_str = format_time(estimated_remaining)


                log_and_save(epoch, step, total_steps, elapsed_str, remaining_str, 
                    avg_recon_local.item(), 
                    avg_break_local.item(), 
                    avg_total_local.item(), 
                    avg_comit_local.item(), 
                    avg_diver_local.item(), 
                    avg_ortho_local.item(), 
                    codebook_usage, 
                    loss_csv_path, rank
                )
                # æ ¼å¼åŒ–ä¸º "MM:SS"
                # âœ… è®¡ç®—å®é™…ç´¯ç§¯çš„æ­¥æ•°
                if (step + 1) % loss_log_interval == 0:
                    actual_steps = loss_log_interval
                else:
                    actual_steps = (step + 1) % loss_log_interval

                if actual_steps <1:
                    actual_steps = 1
                avg_recon_local = step_recon_loss / actual_steps
                avg_break_local = step_break_loss / actual_steps
                avg_total_local = step_total_loss / actual_steps
                avg_comit_local = step_comit_loss / actual_steps
                avg_diver_local = step_diver_loss / actual_steps
                avg_ortho_local = step_ortho_loss / actual_steps

                # æ›´æ–° tqdm åç¼€ï¼ˆåªåœ¨ rank 0 æ˜¾ç¤ºï¼‰
                if rank == 0:
                    dynamic_recon_weight = weights['recon_loss']
                    dynamic_comit_weight = weights['comit_loss']
                    dynamic_diver_weight = 0.0
                    dynamic_ortho_weight = weights['ortho_loss']
                    print(
                        f"[Epoch {epoch+1}/{num_epochs} | Step {steps_done}/{total_steps} | "
                        f"{elapsed_str}<{remaining_str}] "
                        f"Total: {avg_total_local.item():.6f} | "
                        f"Recon: {avg_recon_local.item():.6f} * {dynamic_comit_weight:.3f} | "
                        f"Comit: {avg_comit_local.item():.6f} * {dynamic_comit_weight:.3f} | "
                        f"Diver: {avg_diver_local.item():.6f} * {dynamic_diver_weight:.3f} | "
                        f"Ortho: {avg_ortho_local.item():.6f} * {dynamic_ortho_weight:.3f} | "
                        f"Usage: {codebook_usage:.6f} | "
                    )
                    # âœ…ã€å…³é”®ã€‘è¿½åŠ å†™å…¥ CSVï¼ˆæ¯ interval ä¸€æ¬¡ï¼‰
                    row_data = [
                        epoch + 1,
                        step + 1,
                        avg_recon_local.item(),
                        avg_break_local.item(),
                        avg_total_local.item(),
                        avg_comit_local.item(),
                        avg_diver_local.item(),
                        avg_ortho_local.item(),
                        codebook_usage
                    ]
                    # âœ… è¿½åŠ å†™å…¥ CSV
                    with open(loss_csv_path, 'a', newline='', encoding='utf-8') as f:
                        writer = csv.writer(f)
                        writer.writerow(row_data)

                            # é‡ç½®
                step_recon_loss = torch.tensor(0.0, device=device)
                step_break_loss = torch.tensor(0.0, device=device)
                step_total_loss = torch.tensor(0.0, device=device)
                step_comit_loss = torch.tensor(0.0, device=device)
                step_diver_loss = torch.tensor(0.0, device=device)
                step_ortho_loss = torch.tensor(0.0, device=device)

        # èšåˆæ‰€æœ‰ GPU çš„æŸå¤±ï¼ˆæ±‚å’Œï¼‰
        dist.all_reduce(total_recon_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_break_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_total_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_comit_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_diver_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_ortho_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(num_batches, op=dist.ReduceOp.SUM)

        # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆåªåœ¨ rank 0 æ‰“å°ï¼‰
        avg_recon = total_recon_loss.item() / num_batches.item()
        avg_break = total_break_loss.item() / num_batches.item()
        avg_total = total_total_loss.item() / num_batches.item()
        avg_comit = total_comit_loss.item() / num_batches.item()
        avg_diver = total_diver_loss.item() / num_batches.item()
        avg_ortho = total_ortho_loss.item() / num_batches.item()

        if rank == 0:
            if do_evaluate and epoch < num_epochs - 1:
                codebook_usage, total_tokens = evaluate_codebook_usage()
                print(
                    f"Epoch {epoch+1} - "
                    f"Recon Loss: {avg_recon:.6f} | "
                    f"Comit Loss: {avg_comit:.6f} | "
                    f"Diver Loss: {avg_diver:.6f} | "
                    f"Ortho Loss: {avg_ortho:.6f} | "
                    f"Total Loss: {avg_total:.6f} | "
                    f"Codebook Usage: {codebook_usage:.1%} "
                )

            # âœ… æ£€æŸ¥ç‚¹ä¿å­˜é€»è¾‘ï¼ˆä»… rank 0ï¼‰
            if (epoch + 1) % save_checkpoint_interval == 0:
                checkpoint_path = f"{output_model_path}.epoch{epoch+1}.pth"
                torch.save(model.module.state_dict(), checkpoint_path)
                print(f"âœ… Checkpoint saved to {checkpoint_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆä»… rank 0ï¼‰
    if rank == 0:
        torch.save(model.module.state_dict(), output_model_path)
        print(f"âœ… Final model saved to {output_model_path}")

    dist.barrier()
    dist.destroy_process_group()
