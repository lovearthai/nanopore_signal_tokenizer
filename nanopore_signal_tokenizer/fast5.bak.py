# nanopore_signal_tokenizer/fast5.py

import warnings
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*")

import os
import numpy as np
import glob
from ont_fast5_api.fast5_interface import get_fast5_file
from .nanopore import nanopore_normalize,nanopore_normalize_local,nanopore_normalize_hybrid,nanopore_filter_noise, nanopore_filter
from scipy.signal import medfilt
from pathos.multiprocessing import ProcessPool
from multiprocessing import cpu_count
import tqdm
from scipy.ndimage import median_filter
class Fast5Dir:
    """
    å°† Nanopore åŸå§‹ .fast5 æ–‡ä»¶æ‰¹é‡è½¬æ¢ä¸ºé¢„å¤„ç†åçš„ chunked .npy æ–‡ä»¶ã€‚

    ğŸ“Œ ä¿¡å·å¤„ç†æµæ°´çº¿ï¼ˆæ‰€æœ‰æ­¥éª¤åœ¨ to_chunks_parallel ä¸­æ§åˆ¶ï¼‰ï¼š
        1. ã€ç¼©æ”¾ã€‘raw â†’ pAï¼›
        2. ã€å½’ä¸€åŒ–ã€‘median-MADï¼ˆå¯é€‰ï¼‰ï¼›
        3. ã€ä¸­å€¼æ»¤æ³¢ã€‘kernel=5ï¼ˆå¯é€‰ï¼‰ï¼›
        4. ã€ä½é€šæ»¤æ³¢ã€‘Butterworthï¼ˆå¯é€‰ï¼‰ï¼›
        5. ã€åˆ†å—ã€‘æ»‘åŠ¨çª—å£ + æœ«å°¾å…œåº• + å¤šå¤´è£å‰ªã€‚

    ğŸ“¦ è¾“å‡ºï¼šæ¯ä¸ª .fast5 â†’ ä¸€ä¸ª .npyï¼Œå†…å®¹ä¸º list[dict]ï¼Œå« read_idã€ä½ç½®ã€chunk_dataã€head_cutã€‚
    """

    def __init__(self, fast5_dir: str, default_fs: int = 5000):
        """
        åˆå§‹åŒ–ç›®å½•å¤„ç†å™¨ã€‚

        Args:
            fast5_dir (str): åŒ…å« .fast5 æ–‡ä»¶çš„ç›®å½•ã€‚
            default_fs (int): é»˜è®¤é‡‡æ ·ç‡ï¼ˆHzï¼‰ï¼Œç”¨äºç¼ºå¤± metadata çš„æƒ…å†µã€‚
        """
        if not os.path.isdir(fast5_dir):
            raise ValueError(f"FAST5 directory does not exist: {fast5_dir}")

        self.fast5_dir = fast5_dir
        self.fast5_files = sorted(glob.glob(os.path.join(fast5_dir, "*.fast5")))
        self.default_fs = default_fs

        if not self.fast5_files:
            raise FileNotFoundError(f"No .fast5 files found in {fast5_dir}")

    @staticmethod
    def get_sampling_rate_from_read(read):
        """ä» FAST5 read ä¸­æå–é‡‡æ ·ç‡ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚"""
        try:
            channel_info = read.handle[read.global_key + 'channel_id'].attrs
            return int(channel_info['sampling_rate'])
        except Exception:
            return None

    def _sliding_window_chunks_with_tail(
        self,
        signal: np.ndarray,
        window_size: int,
        stride: int,
        tail_threshold: int,
    ):
        """
        å¯¹ä¸€ç»´ä¿¡å·è¿›è¡Œæ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼Œå¹¶åœ¨æœ«å°¾ä¸è¶³ä¸€ä¸ªçª—å£ä½†æ»¡è¶³æœ€å°é•¿åº¦æ—¶è¡¥å……ä¸€ä¸ª chunkã€‚

        åˆ‡åˆ†ç­–ç•¥ï¼š
          - ä¸»ä½“ä½¿ç”¨å›ºå®š stride æ»‘åŠ¨ï¼›
          - è‹¥æœ«å°¾å‰©ä½™ç‰‡æ®µé•¿åº¦ â‰¥ tail_thresholdï¼Œåˆ™ä»ä¿¡å·æœ«å°¾å€’æ•° window_size ç‚¹å†åˆ‡ä¸€ä¸ª chunkï¼›
          - é¿å…ä¸æœ€åä¸€ä¸ªæ»‘åŠ¨çª—å£é‡å¤ã€‚

        Args:
            signal (np.ndarray): è¾“å…¥ä¸€ç»´ä¿¡å·ã€‚
            window_size (int): æ¯ä¸ª chunk çš„é•¿åº¦ï¼ˆç‚¹æ•°ï¼‰ã€‚
            stride (int): æ»‘åŠ¨æ­¥é•¿ï¼ˆç‚¹æ•°ï¼‰ã€‚
            tail_threshold (int): è§¦å‘æœ«å°¾è¡¥ chunk çš„æœ€å°å‰©ä½™é•¿åº¦ã€‚

        Returns:
            List[Dict]: æ¯ä¸ªå…ƒç´ åŒ…å« 'chunk_start', 'chunk_end', 'chunk_data'ã€‚
        """
        n_points = len(signal)
        if n_points < window_size:
            return []

        chunks = []
        start = 0
        end = 0
        # ä¸»æ»‘åŠ¨çª—å£å¾ªç¯
        while start + window_size <= n_points:
            end = start + window_size
            chunks.append({
                'chunk_start': start,
                'chunk_end': end,
                'chunk_data': signal[start:end].copy()
            })
            start += stride

        # æœ«å°¾å…œåº•ï¼šè‹¥å‰©ä½™éƒ¨åˆ†è¶³å¤Ÿé•¿ä¸”æœªè¢«è¦†ç›–ï¼Œåˆ™ä»æœ«å°¾åˆ‡ä¸€ä¸ªå®Œæ•´çª—å£
        if n_points - end  >= tail_threshold:
            chunks.append({
                'chunk_start': start,
                'chunk_end': n_points,
                'chunk_data': signal[n_points-window_size:n_points].copy()
            })

        return chunks

    def _process_single_fast5(
        self,
        fast5_path: str,
        output_dir: str,
        window_size: int,
        stride: int,
        do_normalize: bool,
        do_medianfilter: bool,
        do_lowpassfilter: bool,
        cut_head_all: int,
        cut_head_step: int,
        tail_threshold: int,
        max_chunks_per_file: int = 100000,
        signal_min_value: int = -1000,
        signal_max_value: int = 1000,
        normal_min_value: int = -10,
        normal_max_value: int = 10
    ):

        NORM_SIG_MIN = normal_min_value
        NORM_SIG_MAX = normal_max_value

        """
        å¤„ç†å•ä¸ª FAST5 æ–‡ä»¶ï¼Œå°† chunks æŒ‰æ•°é‡åˆ†ç‰‡ä¿å­˜ã€‚
        å½“ç´¯è®¡ chunk æ•° â‰¥ max_chunks_per_file æ—¶ï¼Œä¿å­˜ä¸º {basename}_part{N}.npyã€‚
        æœ€ç»ˆå‰©ä½™éƒ¨åˆ†ä¹Ÿä¼šä¿å­˜ï¼ˆå¯èƒ½å°‘äº max_chunks_per_fileï¼‰ã€‚

        Args:
            ...ï¼ˆåŸæœ‰å‚æ•°ä¸å˜ï¼‰...
            max_chunks_per_file (int): æ¯ä¸ªè¾“å‡ºæ–‡ä»¶æœ€å¤§ chunk æ•°é‡ï¼Œé»˜è®¤ 10000ã€‚
        """
        os.makedirs(output_dir, exist_ok=True)
        basename = os.path.basename(fast5_path).rsplit('.', 1)[0]
        buffer = []
        part_idx = 0

        try:
            with get_fast5_file(fast5_path, mode="r") as f5:
                read_ids = f5.get_read_ids()
                if not read_ids:
                    print(f"âš ï¸ No reads found in {fast5_path}")
                    return

                # å¯é€‰ï¼šåŠ  tqdmï¼ˆéœ€ from tqdm import tqdmï¼‰
                reads = list(f5.get_reads())
                for read in reads:
                    # --- ä¿¡å·é¢„å¤„ç†ï¼ˆåŒå‰ï¼‰---
                    # 
                    try:
                        channel_info = read.handle[read.global_key + 'channel_id'].attrs
                        offset = int(channel_info['offset'])
                        scaling = channel_info['range'] / channel_info['digitisation']
                        raw = read.handle[read.raw_dataset_name][:]
                        signal_raw = np.array(scaling * (raw + offset), dtype=np.float32)
                    except Exception as e:
                        print(f"âš ï¸ Failed to extract signal for read {read.read_id}: {e}, skipped.")
                        continue
                    # medf5è¿‡æ»¤ä¸æ‰æœ‰äº›å™ªéŸ³
                    # [456.0, 1341.0, 1341.0, 1341.0, 456.0, 33.0, 33.0]
                    # [113.0, 32767.0, 32767.0, 32767.0, 41.0, 41.0, 41.0]
                    # éœ€è¦ç”¨nanopore_filter_noiseæ¥è¿‡æ»¤
                    if np.any(signal_raw < signal_min_value) or np.any(signal_raw > signal_max_value):
                        signal_clr = nanopore_filter_noise(signal_raw,signal_min_value,signal_max_value) 
                    else:
                        signal_clr = signal_raw

                    #if do_medianfilter:
                    signal_med = medfilt(signal_clr, kernel_size=5).astype(np.float32)
                    # æ£€æŸ¥ä¿¡å·å€¼æ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…
                    if np.any(signal_med < signal_min_value) or np.any(signal_med > signal_max_value):
                        actual_min = signal_med.min()
                        actual_max = signal_med.max()
                        print(f"âš ï¸ Ignored read {fast5_path} {read.read_id} due to out-of-range signal values. "
                              f"Actual range: [{actual_min:.3f}, {actual_max:.3f}], "
                              f"Allowed: [{signal_min_value}, {signal_max_value}]")
                        # æ‰¾å‡ºæ‰€æœ‰å¼‚å¸¸ç‚¹çš„ç´¢å¼•
                        outlier_mask = (signal_med < signal_min_value) | (signal_med > signal_max_value)
                        outlier_indices = np.where(outlier_mask)[0]
                        # åªæ‰“å°å‰å‡ ä¸ªå¼‚å¸¸ç‚¹ï¼ˆé¿å…åˆ·å±ï¼‰
                        max_print = 3
                        for i, idx in enumerate(outlier_indices[:max_print]):
                            start = max(0, idx - 3)
                            end = min(len(signal_med), idx + 4)  # idx+4 å› ä¸ºåˆ‡ç‰‡æ˜¯å·¦é—­å³å¼€
                            context = signal_med[start:end]
                            positions = np.arange(start, end)
                            print(f"  â†’ Outlier #{i+1} at index {idx}: value = {signal_clr[idx]:.3f}")
                            print(f"    Context ({start}â€“{end-1}): {context.tolist()}")
                        if len(outlier_indices) > max_print:
                            print(f"  â†’ ... and {len(outlier_indices) - max_print} more outliers.")
                        continue  # å¿½ç•¥æ­¤ read å¹¶ç»§ç»­ä¸‹ä¸€ä¸ª


                    if do_normalize:
                        #signal = nanopore_normalize(signal)
                        signal,global_mad = nanopore_normalize_hybrid(signal_med,window_size=5000)
                    else:
                        signal = signal_med
                    # æ£€æŸ¥ä¿¡å·å€¼æ˜¯å¦åœ¨æ ‡å‡†åŒ–å…è®¸èŒƒå›´å†… [NORM_SIG_MIN, NORM_SIG_MAX]
                    if np.any(signal < NORM_SIG_MIN) or np.any(signal > NORM_SIG_MAX):
                        actual_min = signal.min()
                        actual_max = signal.max()
                        print(f"âš ï¸ Ignored read {fast5_path} {read.read_id} due to out-of-range signal values. "
                              f"Actual range: [{actual_min:.3f}, {actual_max:.3f}], "
                              f"Allowed: [{NORM_SIG_MIN}, {NORM_SIG_MAX}]")

                        # æ‰¾å‡ºæ‰€æœ‰å¼‚å¸¸ç‚¹çš„ç´¢å¼•
                        outlier_mask = (signal < NORM_SIG_MIN) | (signal > NORM_SIG_MAX)
                        outlier_indices = np.where(outlier_mask)[0]

                        # åªæ‰“å°å‰å‡ ä¸ªå¼‚å¸¸ç‚¹ï¼ˆé¿å…æ—¥å¿—åˆ·å±ï¼‰
                        max_print = 5
                        for i, idx in enumerate(outlier_indices[:max_print]):
                            start = max(0, idx - 5)
                            end = min(len(signal), idx + 6)
                            context = signal[start:end]
                            context_raw = signal_raw[start:end]
                            context_med = signal_med[start:end]
                            context_clr = signal_clr[start:end]
                            print(f"  â†’ Outlier #{i+1} at index {idx}: value = {signal[idx]:.3f}")
                            print(f"    Context ({start}â€“{end-1}): {[f'{x:.3f}' for x in context]}")
                            print(f"    Raw ({start}â€“{end-1}): {[f'{x:.3f}' for x in context_raw]}")
                            print(f"    Clr ({start}â€“{end-1}): {[f'{x:.3f}' for x in context_clr]}")
                            print(f"    Med ({start}â€“{end-1}): {[f'{x:.3f}' for x in context_med]}")

                        if len(outlier_indices) > max_print:
                            print(f"  â†’ ... and {len(outlier_indices) - max_print} more outliers.")

                        continue  # å¿½ç•¥æ­¤ read å¹¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                    if signal.size == 0 or np.isnan(signal).any():
                        print(f"âš ï¸ Invalid signal after normalization for read {read.read_id}, skipped.")
                        continue

                    if do_lowpassfilter:
                        fs_from_read = self.get_sampling_rate_from_read(read)
                        fs = fs_from_read if fs_from_read is not None else self.default_fs
                        try:
                            filtered_signal = nanopore_filter(signal, fs=fs)
                        except Exception as e:
                            print(f"âš ï¸ Filtering failed for read {read.read_id} (fs={fs}): {e}, skipped.")
                            continue
                        if filtered_signal.size == 0 or np.isnan(filtered_signal).any():
                            print(f"âš ï¸ Invalid signal after filtering for read {read.read_id}, skipped.")
                            continue
                        signal = filtered_signal

                    if len(signal) < window_size:
                        print(f"âš ï¸ Read {read.read_id} too short (<{window_size} points), skipped.")
                        continue

                    max_head = min(cut_head_all, len(signal) - 1)
                    head_cuts = list(range(0, max_head + 1, cut_head_step)) or [0]

                    read_chunks = []
                    for head_cut in head_cuts:
                        if head_cut >= len(signal):
                            continue
                        trimmed_signal = signal[head_cut:]
                        chunks = self._sliding_window_chunks_with_tail(
                            trimmed_signal, window_size, stride, tail_threshold
                        )
                        for ch in chunks:
                            read_chunks.append({
                                'read_id': read.read_id,
                                'head_cut': head_cut,
                                'chunk_start_pos': head_cut + ch['chunk_start'],
                                'chunk_end_pos': head_cut + ch['chunk_end'],
                                'chunk_data': ch['chunk_data']
                            })

                    if read_chunks:
                        buffer.extend(read_chunks)

                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                        if len(buffer) >= max_chunks_per_file:
                            save_path = os.path.join(output_dir, f"{basename}_part{part_idx:05d}.npy")
                            np.save(save_path, buffer[:max_chunks_per_file])
                            print(f"âœ… Saved {len(buffer[:max_chunks_per_file])} chunks to {save_path}")
                            buffer = buffer[max_chunks_per_file:]  # ä¿ç•™æº¢å‡ºéƒ¨åˆ†
                            part_idx += 1
                # å¤„ç†å‰©ä½™ buffer
                if buffer:
                    save_path = os.path.join(output_dir, f"{basename}_part{part_idx:05d}.npy")
                    np.save(save_path, buffer)
                    print(f"âœ… Saved final {len(buffer)} chunks to {save_path}")
        except Exception as e:
            print(f"âŒ Critical error processing {fast5_path}: {e}")

    def to_chunks(
        self,
        output_dir: str,
        window_size: int = 32,
        stride: int = 8,
        do_normalize: bool = True,
        do_medianfilter: bool = False,
        do_lowpassfilter: bool = False,
        cut_head_all: int = 5,
        cut_head_step: int = 2,
        tail_threshold: int = 16,
        n_jobs: int = -1,
        signal_min_value: int = -1000,
        signal_max_value: int = 1000,
        normal_min_value: int = -10,
        normal_max_value: int = 10
    ):
        """
        å¹¶è¡Œå¤„ç†æ•´ä¸ª FAST5 ç›®å½•ï¼Œç”Ÿæˆ chunked .npy æ–‡ä»¶ã€‚

        ğŸ¯ å¤šå¤´è£å‰ªè¯´æ˜ï¼š
            ä¸ºé€‚é…ä¸‹æ¸¸ CNN çš„ä¸‹é‡‡æ · strideï¼ˆå¦‚ 12ï¼‰ï¼Œéœ€è¦†ç›–æ‰€æœ‰å¯èƒ½çš„è¾“å…¥å¯¹é½ç›¸ä½ã€‚
            é€šè¿‡è®¾ç½® cut_head_all=11, cut_head_step=1ï¼Œå¯ç”Ÿæˆ 12 ç§èµ·å§‹åç§»ï¼ˆ0~11ï¼‰ï¼Œ
            ç¡®ä¿æ¨¡å‹å­¦ä¹ åˆ°å¹³ç§»é²æ£’çš„ token è¡¨ç¤ºã€‚

        ğŸ¯ æœ«å°¾å…œåº•è¯´æ˜ï¼š
            å½“æ»‘åŠ¨çª—å£ç»“æŸåï¼Œè‹¥å‰©ä½™ä¿¡å·é•¿åº¦ â‰¥ tail_thresholdï¼Œ
            åˆ™ä»ä¿¡å·æœ«å°¾å¼ºåˆ¶åˆ‡å‡ºä¸€ä¸ªå®Œæ•´ windowï¼Œé¿å…ä¿¡æ¯æµªè´¹ã€‚

        Args:
            output_dir (str): è¾“å‡ºç›®å½•ã€‚
            window_size (int): æ¯ä¸ª chunk çš„é•¿åº¦ï¼ˆé»˜è®¤ 32ï¼‰ã€‚
            stride (int): æ»‘åŠ¨æ­¥é•¿ï¼ˆé»˜è®¤ 8ï¼‰ã€‚
            do_normalize (bool): æ˜¯å¦æ‰§è¡Œ median-MAD å½’ä¸€åŒ–ã€‚
            do_medianfilter (bool): æ˜¯å¦åº”ç”¨ä¸­å€¼æ»¤æ³¢ã€‚
            do_lowpassfilter (bool): æ˜¯å¦åº”ç”¨ä½é€šæ»¤æ³¢ã€‚
            cut_head_all (int): æœ€å¤§å¼€å¤´è£å‰ªé•¿åº¦ï¼ˆinclusiveï¼‰ï¼Œå»ºè®®è®¾ä¸º stride-1ã€‚
            cut_head_step (int): è£å‰ªæ­¥é•¿ï¼Œæ§åˆ¶ç›¸ä½è¦†ç›–å¯†åº¦ã€‚
            tail_threshold (int): æœ«å°¾æœ€å°ä¿ç•™ç‚¹æ•°ï¼Œç”¨äºå†³å®šæ˜¯å¦è¡¥ chunkã€‚
            n_jobs (int): å¹¶è¡Œè¿›ç¨‹æ•°ã€‚-1 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ CPU æ ¸å¿ƒã€‚
        """
        os.makedirs(output_dir, exist_ok=True)

        if n_jobs == -1:
            n_jobs = cpu_count()

        # æ—¥å¿—è¾“å‡º
        head_cuts_preview = list(range(0, min(cut_head_all + 1, 20), cut_head_step))  # é˜²æ­¢æ‰“å°è¿‡é•¿
        if cut_head_all >= 20:
            head_cuts_preview.append("...")

        print(f"ğŸ“ Processing {len(self.fast5_files)} FAST5 files from: {self.fast5_dir}")
        print(f"ParallelGroup: using {n_jobs} processes")
        print(f"âš™ï¸  Signal pipeline:")
        print(f"    - Normalize: {'ON' if do_normalize else 'OFF'}")
        print(f"    - Median Filter: {'ON' if do_medianfilter else 'OFF'}")
        print(f"    - Low-pass Filter: {'ON' if do_lowpassfilter else 'OFF'}")
        print(f"    - Head cuts: all={cut_head_all}, step={cut_head_step} â†’ sample phases={head_cuts_preview}")
        print(f"    - Tail threshold: {tail_threshold} (fallback chunk if tail â‰¥ this)")
        print(f"ğŸ’¾ Saving chunks to: {output_dir}")

        # æ„é€ å‚æ•°åˆ—è¡¨
        args_list = [
            (
                fp,
                output_dir,
                window_size,
                stride,
                do_normalize,
                do_medianfilter,
                do_lowpassfilter,
                cut_head_all,
                cut_head_step,
                tail_threshold,
                signal_min_value,
                signal_max_value,
                normal_min_value,
                normal_max_value
            )
            for fp in self.fast5_files
        ]

        # ä½¿ç”¨ pathos å¹¶è¡Œå¤„ç†ï¼ˆæ”¯æŒ pickle ä¸å‹å¥½çš„å¯¹è±¡ï¼‰
        with ProcessPool(nodes=n_jobs) as pool:
            pool.map(self._process_single_fast5_wrapper, args_list)

    def _process_single_fast5_wrapper(self, args):
        """
        ä¾› pathos.multiprocessing è°ƒç”¨çš„å‚æ•°è§£åŒ…åŒ…è£…å™¨ã€‚
        """
        (
            fast5_path,
            output_dir,
            window_size,
            stride,
            do_normalize,
            do_medianfilter,
            do_lowpassfilter,
            cut_head_all,
            cut_head_step,
            tail_threshold,
            signal_min_value,
            signal_max_value,
            normal_min_value,
            normal_max_value
        ) = args
        return self._process_single_fast5(
            fast5_path=fast5_path,
            output_dir=output_dir,
            window_size=window_size,
            stride=stride,
            do_normalize=do_normalize,
            do_medianfilter=do_medianfilter,
            do_lowpassfilter=do_lowpassfilter,
            cut_head_all=cut_head_all,
            cut_head_step=cut_head_step,
            tail_threshold=tail_threshold,
            signal_min_value=signal_min_value,
            signal_max_value=signal_max_value,
            normal_min_value=normal_min_value,
            normal_max_value=normal_max_value
        )
